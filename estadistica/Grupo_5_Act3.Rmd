---
title: "Resolución Actividad 3 máster Bioinformática UNIR (2023)"
author: "Grupo 5"
date: "2024-02-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

```

```{r librerias} 
library(tidyr)
library(stats)
library(FactoMineR)
library(factoextra)
library(tibble)
library(dplyr)

library(glmnet)
```
Para la resolución de la actividad deberemos empezar cargando la base de datos, explorandola y estandarizando todos los datos asegurandonos así de que todos poseen la misma escala. Para ello, en caso de realizar una PCA necesitaremos a cada variable restarle su media y dividir entre la desviación estandar. De esta forma conseguimos una media de 0 y una desviacion estandar de 1.
```{r}
# Carga de los datos y exploración
data_1 <- read.csv("mubio02_act3_alimentos_nutrientes_4900.csv")
head(data_1)[, 1:6]
str(data_1)

# Marcamos la columna "id" como columna idetificadora para futuras referencias
datos_crudos <- data_1  %>% 
  column_to_rownames(var = "id")

# Visualización (datos_crudos)

total_nulos <- sum(is.na(datos_crudos))
datos_sin_nulos <- na.omit(datos_crudos)

# Selección de datos para PCA: se usa SOLO las columnas numéricas, sin columna id ni categóricas
lista_columnas_categoricas <- c("sexo", "estado_civil", 
                   "tabaco", "colesterol", "hdl",
                   "HTA", "hipercolesterolemia", "hipertrigliceridemia", 
                   "ECV_prev", "diab_prev", "hta_prev", 
                   "depres_prev", "FA_prev", "cancer_prev")
```
Seleccionmos las columnas categoricas y las transformamos en valores de tipo factor. Esta transformacion a factor se necesitará mas adelante (regresion logistiica, tablas estratificadas, etc).
```{r}

columnas_categoricas <- datos_sin_nulos[, lista_columnas_categoricas] %>%
  mutate_all(factor)

columnas_numericas <- datos_sin_nulos[, sapply(datos_sin_nulos, is.numeric)] %>%
  select(-one_of(lista_columnas_categoricas))

str(columnas_numericas)

columnas_numericas_estandarizadas <- scale(columnas_numericas, center = TRUE, scale = TRUE)

datos_estandarizados <- cbind(columnas_categoricas, columnas_numericas_estandarizadas)

str(datos_estandarizados)

```
Una vez tenemos todos los datos en la misma escala procedemos a la realización de la PCA.
```{r}
# PCA
datosPCA <- prcomp(columnas_numericas_estandarizadas)
summary(datosPCA)

fviz_screeplot(datosPCA , addlabels = TRUE, ylim = c(0, 15))
```

Segun la regla del codo, escogemos  dos para facilitar la visualizacion y analizamos el PCA.
```{r}
# Análisis PCA
fviz_pca_ind(datosPCA, geom.ind = "point", 
             col.ind = "#FC4E07", 
             axes = c(1, 2), 
             pointsize = 1.5) 

# Scree plot de los PCs

fviz_screeplot(datosPCA, addlabels = TRUE, ylim = c(0, 15))

# Scree plot de las variables

fviz_contrib(datosPCA, choice = "var", axes = 1, top = 10)


# Proporción de varianza explicada por cada componente principal
prop_var_explicada <- (datosPCA$sdev^2) / sum(datosPCA$sdev^2)

# Crear un dataframe con los componentes y su proporción de varianza explicada
df_componentes <- data.frame(Componente = paste0("Componente ", 1:length(prop_var_explicada)),
                              R2 = prop_var_explicada)

# Imprimir el dataframe
print(df_componentes)

```
Este screenplot muestra la varianza explicada para cada componente principal. La PC1 explica un 11.5% de la varianza seguido del PC2 con un 3.8% y el PC3 de 2.7%. Para hacer una representación en 2D nos quedaremos con la PC1 y la PC2 (Individuals PCA). Aun así, esta representación 2D solo representa el 15% aprox (11.8+3.5=15.3%) de la varianza total, por lo que podría haber distorsiones

Establecemos categorías por los terciles de los PCA. Para la regresion logística es más facil representarlo como categorías en una misma columna que como columnas separadas.
separadas

```{r}
# Regresión logística

datosPCA$rotation[,1]
datosPCA$rotation[,2]
datosPCA$rotation[,3]

# seleccionar los 3 primeros componentes
datosPCA$x[, 1:3]

# agregarmos los 3 PC seleccionados al final del dataframe
datos_completos <- cbind(datos_estandarizados, datosPCA$x[, 1:3])

# Creamos un subconjunto con las tres columnas PC1, PC2 y PC3
columnas_pc <- datos_completos[, paste("PC",1:3, sep = "")]
# head(columnas_pc)

for(i in 1:3) {
  columna_terciles <- cut(columnas_pc[[i]], breaks = quantile(columnas_pc[[i]], probs = seq(0, 1, by = 1/3)), labels = c("1", "2", "3"), include.lowest = TRUE)
  datos_completos[paste(names(columnas_pc)[i], "tercil", sep = "_")] <- columna_terciles}
```
**Interpretación:**
Los coeficientes para PC1, tienen valores P comparativamente altos, especialmente para el tercil 2. Sin embargo, los de PC2 tienen una alta significancia (valores P muy pequeños). Por tanto, podemos establecer que las variables agrupadas bajo el componente PC2 tienen mayor relacion con la variable objetivo.

Así mismo, el sexo parece tener una gran influencia sobre la incidencia de diabetes. En concreto, pertenecer al sexo etiquetado como "2" está inversamente relacionado con la probabilidad de desarrollar diabetes.

Dada la baja influencia del PC1 para nuestro análisis, incluimos el PC3, para comprobar si tienen o no incidencia: 
```{r}
# Regresión logística
modelo_logistica <- glm(diab_prev ~ PC2_tercil + PC3_tercil + sexo, data = datos_completos, family = binomial)
summary(modelo_logistica)
```
**Interpretación:**
Los coeficientes para PC3, tienen valores P muy bajos, lo que implica que este componente tiene una alta significación para la prevaliencia de diabetes.
En este caso, parece que la relación es directamente proporcional, como en el caso de PC2.

De momento, este es el modelo que seleccionamos, ya que las tres variables tienen una alta significación para la variable objetivo.

```{r }
# Cálculo de la odds ratio y visualización en una única tabla
exp(cbind("Odds Ratio" =  coef(modelo_logistica),  confint(modelo_logistica)))
```

Si nos basamos en estos valores, hemos obtenido unos coeﬁcientes positivos para los dos PC: a medida que aumenta su valor, mayor es el efecto de dichas variables sobre la respuesta.
En el caso de la variable sexo, al tener unos valores de intervalo de confianza por debajo de uno, combinado con el odds ratio menor que uno, confirma que el sexo  influye negativamente en la probabilidad de desarrollar diabetes.

